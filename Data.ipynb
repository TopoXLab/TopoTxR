{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run include.ipynb\n",
    "%run FileIO.ipynb\n",
    "%run Medical_IO.ipynb\n",
    "import glob\n",
    "import cv2\n",
    "from scipy import signal\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "class Data_topo(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, root, root_pds, target_type, transform=None):\n",
    "        self.root             = root\n",
    "        self.transform        = transform\n",
    "        self.address_book     = []\n",
    "        self.address_book_pds = []\n",
    "        os.chdir(root)\n",
    "        for file in glob.glob(\"*.\"+target_type):\n",
    "            self.address_book.append(os.path.join(root, file))\n",
    "            self.address_book_pds.append(os.path.join(root_pds, file+\".dat\"))\n",
    "        img_tease = cv2.imread(self.address_book[0], cv2.IMREAD_GRAYSCALE)\n",
    "        print(\"Image shape: \" + str(img_tease.shape))\n",
    "        print(\"Image value range: %.2f - %.2f\" %(np.amin(img_tease), np.amax(img_tease)))\n",
    "        print(\"Image data type\" + str(type(img_tease[0][0])))\n",
    "        print(\"Required data type is np.uint8\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.address_book)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        img = np.uint8(cv2.imread(self.address_book[idx], cv2.IMREAD_GRAYSCALE))\n",
    "        pd_path = self.address_book_pds[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        instance = {'image': img, 'pd_path': pd_path}\n",
    "        return instance\n",
    "    \n",
    "class NII_with_label(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, root, target_type, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.address_book = []\n",
    "        self.labels = []\n",
    "        self.kernel = Util_gen.generate_gaussian_kernel(3, 2, 3)\n",
    "        \n",
    "        os.chdir(root)\n",
    "        for file in glob.glob(\"*.\"+target_type):\n",
    "            self.address_book.append(os.path.join(root, file))\n",
    "            self.labels.append(int(file.split('_')[2]))\n",
    "        if (target_type == 'nii'):\n",
    "            tease = FileIO_MEDICAL.load_nii(self.address_book[0])\n",
    "        else:\n",
    "            print(\"Data_with_label: unrecognized data type\")\n",
    "        print(\"Image shape: \" + str(tease.shape))\n",
    "        print(\"Image value range: %.2f - %.2f\" %(np.amin(tease), np.amax(tease)))\n",
    "        print(\"Image data type\" + str(type(tease[0][0][0])))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.address_book)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        vol = np.float32(FileIO_MEDICAL.load_nii(self.address_book[idx]))\n",
    "        #vol = signal.convolve(vol, self.kernel, mode='same')\n",
    "        if self.transform:\n",
    "            vol = self.transform(vol)\n",
    "        instance = {'vol': vol, 'label': self.labels[idx]}\n",
    "        return instance\n",
    "    \n",
    "class Data_dmt(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, root, target_type, transform=None):\n",
    "        self.root             = root\n",
    "        self.transform        = transform\n",
    "        self.address_book     = []\n",
    "        os.chdir(root)\n",
    "        for file in glob.glob(\"*.\"+target_type):\n",
    "            self.address_book.append(os.path.join(root, file))\n",
    "        img_tease = FileIO.read_matrix_binary(self.address_book[0], 'i')\n",
    "        print(\"Image shape: \" + str(img_tease.shape))\n",
    "        print(\"Image value range: %.2f - %.2f\" %(np.amin(img_tease), np.amax(img_tease)))\n",
    "        print(\"Image data type\" + str(type(img_tease[0][0])))\n",
    "        print(\"Required data type is np.int\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.address_book)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        img = np.float32(FileIO.read_matrix_binary(self.address_book[idx], 'i')) - 1.0\n",
    "        img = (img - np.amax(img)/2.0) / (np.amax(img)/2.0)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            img = torch.tanh(img)\n",
    "        return img\n",
    "    \n",
    "class Data_fetcher(object):\n",
    "    \n",
    "    @staticmethod\n",
    "    def fetch_dataset(name, data_path, batch_size, batch_workers, shuffle, drop_last, scalor, test_split=0.0, random_seed=-1):\n",
    "        if name == \"cifar10\":\n",
    "            dataset = dset.CIFAR10(root=data_path, download=True,\n",
    "                      transform=transforms.Compose([\n",
    "                          transforms.Resize([64, 64]),\n",
    "                          transforms.ToTensor(),\n",
    "                          transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                      ]))\n",
    "        elif name == \"celeba\": # The data should be under a folder under root: root/celeba/*.png\n",
    "            dataset = dset.ImageFolder(root=data_path,\n",
    "                      transform=transforms.Compose([\n",
    "                          transforms.Resize(64),\n",
    "                          transforms.CenterCrop(64),\n",
    "                          transforms.ToTensor(),\n",
    "                          transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                      ]))\n",
    "        elif name == \"topo\":\n",
    "            dataset = Data_topo(data_path, FLAGS.pds_path, FLAGS.data_extension,\n",
    "                  transform=transforms.Compose(\n",
    "                 [transforms.ToPILImage(),\n",
    "                  transforms.ToTensor(),\n",
    "                  transforms.Normalize([scalor], [scalor])\n",
    "                 ]))\n",
    "        elif name == \"dmt\":\n",
    "            dataset = Data_dmt(data_path, FLAGS.data_extension,\n",
    "                  transform=transforms.Compose(\n",
    "                 [transforms.ToPILImage(),\n",
    "                  transforms.ToTensor()\n",
    "                 ]))\n",
    "        elif name == \"nii\":\n",
    "                dataset = NII_with_label(data_path, FLAGS.data_extension,\n",
    "                      transform=transforms.Compose(\n",
    "                     [transforms.ToTensor()\n",
    "                     ]))\n",
    "        else:\n",
    "            raise NotImplementedError('Unrecognized dataset %s' % name)\n",
    "        \n",
    "#         dataset_size = len(dataset)\n",
    "#         split = int(np.floor(test_split*dataset_size))\n",
    "#         train_, test_ = torch.utils.data.random_split(dataset, [dataset_size-split, split])\n",
    "#         train_loader = torch.utils.data.DataLoader(train_, batch_size=batch_size,\n",
    "#              shuffle=shuffle, num_workers=int(batch_workers), drop_last=drop_last)\n",
    "#         test_loader = torch.utils.data.DataLoader(test_, batch_size=batch_size,\n",
    "#              shuffle=shuffle, num_workers=int(batch_workers), drop_last=drop_last)\n",
    "#         return train_loader, test_loader\n",
    "           \n",
    "        if (test_split > 0.0):\n",
    "            dataset_size = len(dataset)\n",
    "            indices = list(range(dataset_size))\n",
    "            split = int(np.floor(test_split*dataset_size))\n",
    "            if shuffle:\n",
    "                np.random.seed(random_seed)\n",
    "                np.random.shuffle(indices)\n",
    "            train_indices, test_indices = indices[split:], indices[:split]\n",
    "\n",
    "            train_sampler = SubsetRandomSampler(train_indices)\n",
    "            test_sampler  = SubsetRandomSampler(test_indices)\n",
    "            train_loader  = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "            test_loader   = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)\n",
    "            return train_loader, test_loader\n",
    "        else:\n",
    "            dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                         shuffle=shuffle, num_workers=int(batch_workers), drop_last=drop_last)\n",
    "            return dataloader\n",
    "        \n",
    "    @staticmethod\n",
    "    def fetch_dataset_wValidation(name, data_path, batch_size, batch_workers, shuffle, drop_last, scalor, datasplit_scheme, test_split, xfold, fold_idx, random_seed=-1):\n",
    "        '''\n",
    "        This is the advanced version to fetch_dataset with validation split, it works like this:\n",
    "        the data is split into [train+validation][test] according to test_split\n",
    "        the [train+validation] is further split into [train][validation] according to valid_split\n",
    "        @name: name of the dataset\n",
    "        @batch_size: number of instances per batch\n",
    "        @batch_workers: number of workers to fetch data\n",
    "        @shuffle: if to shuffle the data\n",
    "        @drop_last: drop the instances that do not fit in the last batch\n",
    "        @scalor: scale the data\n",
    "        @datasplit_scheme: \"All\" use all data for training; \"Test\" partitions into train/test according to test_split; \"Valid\" paritions into train/test/validation\n",
    "        @test_split: percentage of the data for test\n",
    "        @xfold: number of folds for validation\n",
    "        @fold_idx: x-fold cross validation, indicates which fold to use as validation\n",
    "        @random_seed: for random indices shuffle purpose\n",
    "        '''\n",
    "        if name == \"cifar10\":\n",
    "            dataset = dset.CIFAR10(root=data_path, download=True,\n",
    "                      transform=transforms.Compose([\n",
    "                          transforms.Resize([64, 64]),\n",
    "                          transforms.ToTensor(),\n",
    "                          transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                      ]))\n",
    "        elif name == \"celeba\": # The data should be under a folder under root: root/celeba/*.png\n",
    "            dataset = dset.ImageFolder(root=data_path,\n",
    "                      transform=transforms.Compose([\n",
    "                          transforms.Resize(64),\n",
    "                          transforms.CenterCrop(64),\n",
    "                          transforms.ToTensor(),\n",
    "                          transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                      ]))\n",
    "        elif name == \"topo\":\n",
    "            dataset = Data_topo(data_path, FLAGS.pds_path, FLAGS.data_extension,\n",
    "                  transform=transforms.Compose(\n",
    "                 [transforms.ToPILImage(),\n",
    "                  transforms.ToTensor(),\n",
    "                  transforms.Normalize([scalor], [scalor])\n",
    "                 ]))\n",
    "        elif name == \"dmt\":\n",
    "            dataset = Data_dmt(data_path, FLAGS.data_extension,\n",
    "                  transform=transforms.Compose(\n",
    "                 [transforms.ToPILImage(),\n",
    "                  transforms.ToTensor()\n",
    "                 ]))\n",
    "        elif name == \"nii\":\n",
    "            dataset = NII_with_label(data_path, FLAGS.data_extension,\n",
    "                  transform=transforms.Compose(\n",
    "                 [transforms.ToTensor()\n",
    "                 ]))\n",
    "        else:\n",
    "            raise NotImplementedError('Unrecognized dataset %s' % name)\n",
    "        \n",
    "#         dataset_size = len(dataset)\n",
    "#         split = int(np.floor(test_split*dataset_size))\n",
    "#         train_, test_ = torch.utils.data.random_split(dataset, [dataset_size-split, split])\n",
    "#         train_loader = torch.utils.data.DataLoader(train_, batch_size=batch_size,\n",
    "#              shuffle=shuffle, num_workers=int(batch_workers), drop_last=drop_last)\n",
    "#         test_loader = torch.utils.data.DataLoader(test_, batch_size=batch_size,\n",
    "#              shuffle=shuffle, num_workers=int(batch_workers), drop_last=drop_last)\n",
    "#         return train_loader, test_loader\n",
    "\n",
    "        if datasplit_scheme==\"All\":\n",
    "            print(\"Using all data for trianing in data fetcher.\")\n",
    "            dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                         shuffle=shuffle, num_workers=int(batch_workers), drop_last=drop_last)\n",
    "            return dataloader\n",
    "        elif datasplit_scheme == \"Test\":\n",
    "            print(\"Test mode in data fetcher.\")\n",
    "            dataset_size = len(dataset)\n",
    "            indices = list(range(dataset_size))\n",
    "            if shuffle:\n",
    "                np.random.seed(random_seed)\n",
    "                np.random.shuffle(indices)\n",
    "            fold_base = int(np.floor(dataset_size)/xfold)\n",
    "            fold_rec = [None] * xfold\n",
    "            for fold_gen in range(xfold-1):\n",
    "                list_tmp = list(np.arange(fold_gen*fold_base, (fold_gen+1)*fold_base, dtype=np.int32))\n",
    "                fold_rec[fold_gen] = list_tmp\n",
    "            fold_rec[xfold-1] = list(np.arange((xfold-1)*fold_base, dataset_size, dtype=np.int32))\n",
    "            \n",
    "            assert(fold_idx < xfold)\n",
    "            fold_train = list()\n",
    "            for fold_gen in range(xfold):\n",
    "                if fold_gen == fold_idx:\n",
    "                    continue\n",
    "                else:\n",
    "                    fold_train = fold_train + fold_rec[fold_gen]\n",
    "            fold_test = fold_rec[fold_idx]\n",
    "            \n",
    "            indices = np.asarray(indices)\n",
    "            train_indices = list(indices[fold_train])\n",
    "            test_indices  = list(indices[fold_test])\n",
    "            \n",
    "            train_sampler = SubsetRandomSampler(train_indices)\n",
    "            test_sampler  = SubsetRandomSampler(test_indices)\n",
    "            train_loader  = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "            test_loader   = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)\n",
    "            return train_loader, test_loader\n",
    "        elif datasplit_scheme == \"Valid\":\n",
    "            print(\"Validation mode in data fetcher.\")\n",
    "            dataset_size = len(dataset)\n",
    "            indices = list(range(dataset_size))\n",
    "            split = int(np.floor(test_split*dataset_size))\n",
    "            if shuffle:\n",
    "                np.random.seed(random_seed)\n",
    "                np.random.shuffle(indices)\n",
    "            test_indices = indices[dataset_size-split:]\n",
    "            train_valid_indices = indices[:dataset_size-split]\n",
    "            \n",
    "            fold_base = int(np.floor((dataset_size - split)/xfold))\n",
    "            fold_rec = [None]*xfold\n",
    "            for fold_gen in range(xfold-1):\n",
    "                list_tmp = list(np.arange(fold_gen*fold_base, (fold_gen+1)*fold_base, dtype=np.int32))\n",
    "                fold_rec[fold_gen] = list_tmp\n",
    "            fold_rec[xfold-1] = list(np.arange((xfold-1)*fold_base, dataset_size-split, dtype=np.int32))\n",
    "            \n",
    "            assert(fold_idx < xfold)\n",
    "            fold_train = list()\n",
    "            for fold_gen in range(xfold):\n",
    "                if fold_gen == fold_idx:\n",
    "                    continue\n",
    "                else:\n",
    "                    fold_train = fold_train + fold_rec[fold_gen]\n",
    "            fold_valid = fold_rec[fold_idx]\n",
    "            \n",
    "            train_valid_indices = np.asarray(train_valid_indices)\n",
    "            train_indices = list(train_valid_indices[fold_train])\n",
    "            valid_indices = list(train_valid_indices[fold_valid])\n",
    "\n",
    "            train_sampler = SubsetRandomSampler(train_indices)\n",
    "            valid_sampler = SubsetRandomSampler(valid_indices)\n",
    "            test_sampler  = SubsetRandomSampler(test_indices)\n",
    "            train_loader  = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "            valid_loader  = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=valid_sampler)\n",
    "            test_loader   = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)\n",
    "            return train_loader, valid_loader, test_loader"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
