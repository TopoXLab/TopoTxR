{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import sklearn.metrics\n",
    "import numpy.random\n",
    "import pymrmr\n",
    "import scipy.stats as ss\n",
    "\n",
    "class ML_DataLoader(object):\n",
    "    np.random.seed(1)\n",
    "    def __init__(self,data_path,label_path):\n",
    "        self.data_path=data_path\n",
    "        self.label_path = label_path\n",
    "        self.full_data=pd.read_excel(data_path)\n",
    "        self.full_label=pd.read_excel(label_path)\n",
    "        \n",
    "        self.data=self.full_data.iloc[:,1::].to_numpy()\n",
    "        self.labels=self.full_label.iloc[:,1].to_numpy()\n",
    "        \n",
    "        self.positive_indices=[]\n",
    "        self.negative_indices=[]\n",
    "        self.positive_test_indices=[]\n",
    "        self.positive_train_indices=[]\n",
    "        self.negative_test_indices=[]\n",
    "        self.negative_train_indices=[]\n",
    "        \n",
    "    def createFolds(self,n_folds):\n",
    "        \n",
    "        pos_is_maj=0\n",
    "        self.positive_indices=np.where(self.labels==1)[0]\n",
    "        self.negative_indices=np.where(self.labels==0)[0]\n",
    "        if self.positive_indices.shape[0]>self.negative_indices.shape[0]:\n",
    "            pos_is_maj=1\n",
    "            \n",
    "        if pos_is_maj:\n",
    "            one_fold_size=self.negative_indices.shape[0]/n_folds   \n",
    "        else:\n",
    "            one_fold_size=self.positive_indices.shape[0]/n_folds\n",
    "        train_size=np.math.ceil((n_folds-1)*one_fold_size)\n",
    "\n",
    "        self.positive_train_indices=np.random.choice(self.positive_indices,[train_size,1],replace=False)[:,0]\n",
    "        self.negative_train_indices=np.random.choice(self.negative_indices,[train_size,1],replace=False)[:,0]       \n",
    "        self.positive_test_indices=np.setdiff1d(np.union1d(self.positive_train_indices,self.positive_indices),np.intersect1d(self.positive_train_indices,self.positive_indices))\n",
    "        self.negative_test_indices=np.setdiff1d(np.union1d(self.negative_train_indices,self.negative_indices),np.intersect1d(self.negative_train_indices,self.negative_indices))\n",
    "        if pos_is_maj:\n",
    "            self.positive_test_indices=np.random.choice(self.positive_test_indices,self.negative_test_indices.size,replace=False)\n",
    "        elif not pos_is_maj:\n",
    "            self.negative_test_indices=np.random.choice(self.negative_test_indices,self.positive_test_indices.size,replace=False)       \n",
    "        \n",
    "        featnames=self.full_data.columns[1::]\n",
    "        labelnames=[]\n",
    "        labelnames.append(self.full_label.columns[1])\n",
    "\n",
    "        train=np.concatenate((self.data[self.positive_train_indices,:],self.data[self.negative_train_indices,:]),0)\n",
    "        test=np.concatenate((self.data[self.positive_test_indices,:],self.data[self.negative_test_indices,:]),0)\n",
    "        train_labels=np.concatenate((self.labels[self.positive_train_indices],self.labels[self.negative_train_indices]),0)\n",
    "        test_labels=np.concatenate((self.labels[self.positive_test_indices],self.labels[self.negative_test_indices]),0)\n",
    "        \n",
    "        train=pd.DataFrame(train,columns=featnames)\n",
    "        test=pd.DataFrame(test,columns=featnames)\n",
    "        train_labels=pd.DataFrame(train_labels,columns=labelnames)\n",
    "        test_labels=pd.DataFrame(test_labels,columns=labelnames)\n",
    "        return train,test,train_labels,test_labels  \n",
    "    \n",
    "def RemoveCorrFeatures(data, thresh=.9):\n",
    "    corr=data.corr()\n",
    "    columns = np.full((corr.shape[0],), True, dtype=bool)\n",
    "    for i in range(corr.shape[0]):\n",
    "        for j in range(i+1, corr.shape[0]):\n",
    "            if corr.iloc[i,j] >= thresh:\n",
    "                if columns[j]:\n",
    "                    columns[j] = False\n",
    "    selected_columns = data.columns[columns]\n",
    "    data = data[selected_columns]\n",
    "    return data\n",
    "\n",
    "\n",
    "def PerformFeatureSelection(Data, Labels, n=5, Methods=(\"MID\",\"MIQ\",\"ranksum\",\"ttest\"), p=.05):\n",
    "    \"\"\" \n",
    "        Takes a dataframe as input and returns dict of features\n",
    "        Uses the 'full_data' attribute of ML_DataLoader. \n",
    "    \"\"\"\n",
    "  \n",
    "    DesiredFeatureNames=dict()\n",
    "    mrmrData=Labels.join(Data)\n",
    "    for column in mrmrData.columns:\n",
    "        thresh=mrmrData[column].mean()\n",
    "        mrmrData[column]=(mrmrData[column]>=thresh).astype(int)\n",
    "    \n",
    "    if \"MID\" in Methods:\n",
    "        DesiredFeatureNames[\"MID\"]=np.asarray(pymrmr.mRMR(mrmrData,\"MID\",n)).astype(str)\n",
    "    if \"MIQ\" in Methods:  \n",
    "        DesiredFeatureNames[\"MIQ\"]=np.asarray(pymrmr.mRMR(mrmrData,\"MIQ\",n)).astype(str)\n",
    "    if \"ranksum\" in Methods:\n",
    "        Data=RemoveCorrFeatures(Data)\n",
    "        p_values=[]\n",
    "        featurenames=[]\n",
    "        positiveClass=Data.loc[Labels.iloc[:,0]==1,:]\n",
    "        #print(positiveClass)\n",
    "        negativeClass=Data.loc[Labels.iloc[:,0]==0,:]\n",
    "        #print(negativeClass)\n",
    "        for i in range(positiveClass.shape[1]):\n",
    "            calc_p=ss.ranksums(positiveClass.iloc[:,i],negativeClass.iloc[:,i])[1]\n",
    "            if calc_p<=p:\n",
    "                p_values.append(calc_p)\n",
    "                featurenames.append(positiveClass.columns[i])\n",
    "        d={'p_values':p_values,'featnames':featurenames}\n",
    "        ranksumdf=pd.DataFrame(d)\n",
    "        sorteddf=ranksumdf.sort_values(by='p_values')\n",
    "        DesiredFeatureNames[\"ranksum\"]=sorteddf.iloc[0:n,1].to_numpy().astype(str)\n",
    "\n",
    "    if \"ttest\" in Methods:\n",
    "        Data=RemoveCorrFeatures(Data)\n",
    "        p_values=[]\n",
    "        featurenames=[]\n",
    "        positiveClass=Data.loc[Labels.iloc[:,0]==1,:]\n",
    "\n",
    "        negativeClass=Data.loc[Labels.iloc[:,0]==0,:]\n",
    "        for i in range(positiveClass.shape[1]):\n",
    "            calc_p=ss.ttest_ind(positiveClass.iloc[:,i],negativeClass.iloc[:,i])[1]\n",
    "            if calc_p<=p:\n",
    "                p_values.append(calc_p)\n",
    "                featurenames.append(positiveClass.columns[i])\n",
    "        dt={'p_values':p_values,'featnames':featurenames}\n",
    "        ttestdf=pd.DataFrame(dt)\n",
    "        ttestsorteddf=ttestdf.sort_values(by='p_values')\n",
    "        DesiredFeatureNames[\"ttest\"]=ttestsorteddf.iloc[0:n,1].to_numpy().astype(str)\n",
    "\n",
    "    return DesiredFeatureNames\n",
    "\n",
    "path_label = 'E:/Data2/BreastMass_refine/outcome.xlsx'\n",
    "path_feat = 'E:/Data2/BreastMass_refine/radiomic_features/radiomic_features_flag_normalize_false.xlsx'\n",
    "\n",
    "data_loader = ML_DataLoader(path_feat, path_label)\n",
    "iterations=2\n",
    "folds=5\n",
    "desiredClassifiers=['lda','qda']\n",
    "selectionTypes=(\"MID\",\"MIQ\",\"ranksum\",\"ttest\")\n",
    "num_feats=93\n",
    "\n",
    "LDAStats=pd.DataFrame(columns=['sensitivity','sens_std','specificity','spec_std','auc','auc_std'])\n",
    "QDAStats=pd.DataFrame(columns=['sensitivity','sens_std','specificity','spec_std','auc','auc_std'])\n",
    "RFStats=pd.DataFrame(columns=['sensitivity','sens_std','specificity','spec_std','auc','auc_std'])\n",
    "MIDFeats=pd.DataFrame(columns=['MID_Feats','MID_counts'])\n",
    "MIQFeats=pd.DataFrame(columns=[\"MIQ_Feats\",'MIQ_counts'])\n",
    "RanksumFeats=pd.DataFrame(columns=[\"RS_Feats\",'RS_counts'])\n",
    "TtestFeats=pd.DataFrame(columns=[\"Ttest_Feats\",'Ttest_counts'])\n",
    "[train_set,test_set,train_labels,test_labels]=data_loader.createFolds(5) #really lazy to get feature names\n",
    "totalFeatNames=train_set.columns\n",
    "counts=np.zeros(totalFeatNames.shape)\n",
    "td={ 'feats':totalFeatNames, 'counts':counts}\n",
    "MID_featcounts=pd.DataFrame(td,index=td['feats'])\n",
    "MIQ_featcounts=pd.DataFrame(td,index=td['feats'])\n",
    "RS_featcounts=pd.DataFrame(td,index=td['feats'])\n",
    "Ttest_featcounts=pd.DataFrame(td,index=td['feats'])\n",
    "\n",
    "for selection_type in (selectionTypes):\n",
    "    lda_sens_ave=[]\n",
    "    lda_sens_std=[]\n",
    "    lda_spec_ave=[]\n",
    "    lda_spec_std=[]\n",
    "    lda_auc_ave=[]\n",
    "    lda_auc_std=[]\n",
    "\n",
    "    qda_sens_ave=[]\n",
    "    qda_sens_std=[]\n",
    "    qda_spec_ave=[]\n",
    "    qda_spec_std=[]\n",
    "    qda_auc_ave=[]\n",
    "    qda_auc_std=[]\n",
    "\n",
    "\n",
    "    rf_sens_ave=[]\n",
    "    rf_sens_std=[]\n",
    "    rf_spec_ave=[]\n",
    "    rf_spec_std=[]\n",
    "    rf_auc_ave=[]\n",
    "    rf_auc_std=[]\n",
    "\n",
    "    lda_sensitivity=np.ones([num_feats,iterations])\n",
    "\n",
    "    lda_specificity=np.ones([num_feats,iterations])\n",
    "\n",
    "    lda_auc=np.ones([num_feats,iterations])\n",
    "\n",
    "    qda_sensitivity=np.ones([num_feats,iterations])\n",
    "\n",
    "    qda_specificity=np.ones([num_feats,iterations])\n",
    "\n",
    "    qda_auc=np.ones([num_feats,iterations])\n",
    "\n",
    "    rf_sensitivity=np.ones([num_feats,iterations])\n",
    "\n",
    "    rf_specificity=np.ones([num_feats,iterations])\n",
    "\n",
    "    rf_auc=np.ones([num_feats,iterations])\n",
    "\n",
    "    for iteration in range(iterations):\n",
    "        print('iteration',iteration+1)\n",
    "        [train_set,test_set,train_labels,test_labels]=data_loader.createFolds(folds)\n",
    "        feat_dict=PerformFeatureSelection(train_set,train_labels,n=num_feats,Methods=(selection_type))\n",
    "        \n",
    "        if selection_type==\"MID\":\n",
    "            MID_featcounts.loc[feat_dict[selection_type],'counts']=MID_featcounts.loc[feat_dict[selection_type],'counts']+1\n",
    "        if selection_type==\"MIQ\":\n",
    "            MIQ_featcounts.loc[feat_dict[selection_type],'counts']=MIQ_featcounts.loc[feat_dict[selection_type],'counts']+1\n",
    "        if selection_type==\"ranksum\":\n",
    "            RS_featcounts.loc[feat_dict[selection_type],'counts']=RS_featcounts.loc[feat_dict[selection_type],'counts']+1\n",
    "        if selection_type==\"ttest\":\n",
    "            Ttest_featcounts.loc[feat_dict[selection_type],'counts']=Ttest_featcounts.loc[feat_dict[selection_type],'counts']+1\n",
    "        train_labels=np.ravel(train_labels.to_numpy())\n",
    "        test_labels=test_labels.to_numpy()\n",
    "        \n",
    "        for current_feats in range(num_feats):\n",
    "            trimmed_train_set=train_set.loc[:,feat_dict[selection_type][0:current_feats+1]].to_numpy() \n",
    "            trimmed_test_set=test_set.loc[:,feat_dict[selection_type][0:current_feats+1]].to_numpy()\n",
    "            print(trimmed_train_set.shape)\n",
    "            print(trimmed_test_set.shape)\n",
    "            \n",
    "            if 'lda' in desiredClassifiers:\n",
    "                try:\n",
    "                    lda=LinearDiscriminantAnalysis()\n",
    "                    lda_model=lda.fit(trimmed_train_set,train_labels)\n",
    "                    lda_pred_prob=lda_model.predict_proba(trimmed_test_set)\n",
    "                    lda_pred=lda_model.predict(trimmed_test_set)\n",
    "                    lda_conf=confusion_matrix(test_labels,lda_pred)\n",
    "                    lda_sensitivity[current_feats,iteration]=(lda_conf[0,0]/(lda_conf[0,0]+lda_conf[0,1]))\n",
    "                    lda_specificity[current_feats,iteration]=(lda_conf[1,1]/(lda_conf[1,0]+lda_conf[1,1]))\n",
    "                    lda_auc[current_feats,iteration]=(sklearn.metrics.roc_auc_score(test_labels,lda_pred_prob[:,1]))\n",
    "                except:\n",
    "                    lda_sensitivity[current_feats,iteration]=float('nan')\n",
    "                    lda_specificity[current_feats,iteration]=float('nan')\n",
    "                    lda_auc[current_feats,iteration]=float('nan')\n",
    "            if 'qda' in desiredClassifiers:\n",
    "                try:\n",
    "                    qda=QuadraticDiscriminantAnalysis()\n",
    "                    qda_model=qda.fit(trimmed_train_set,train_labels)\n",
    "                    qda_pred_prob=qda_model.predict_proba(trimmed_test_set)\n",
    "                    qda_pred=qda_model.predict(trimmed_test_set)\n",
    "                    qda_conf=confusion_matrix(test_labels,qda_pred)\n",
    "                    qda_sensitivity[current_feats,iteration]=(qda_conf[0,0]/(qda_conf[0,0]+qda_conf[0,1]))\n",
    "                    qda_specificity[current_feats,iteration]=(qda_conf[1,1]/(qda_conf[1,0]+qda_conf[1,1]))\n",
    "                    qda_auc[current_feats,iteration]=(sklearn.metrics.roc_auc_score(test_labels,qda_pred_prob[:,1]))\n",
    "                except:\n",
    "                    qda_sensitivity[current_feats,iteration]=float('nan')\n",
    "                    qda_specificity[current_feats,iteration]=float('nan')\n",
    "                    qda_auc[current_feats,iteration]=float('nan')\n",
    "            if 'rf' in desiredClassifiers:        \n",
    "                rf=RandomForestClassifier(n_estimators=100)\n",
    "                rf.fit(trimmed_train_set,train_labels)\n",
    "                rf_pred_prob=rf.predict_proba(trimmed_test_set)\n",
    "                rf_pred=rf.predict(trimmed_test_set)\n",
    "                rf_conf=confusion_matrix(test_labels,rf_pred)\n",
    "                rf_sensitivity[current_feats,iteration]=(rf_conf[0,0]/(rf_conf[0,0]+rf_conf[0,1]))\n",
    "                rf_specificity[current_feats,iteration]=(rf_conf[1,1]/(rf_conf[1,0]+rf_conf[1,1]))\n",
    "                rf_auc[current_feats,iteration]=(sklearn.metrics.roc_auc_score(test_labels,rf_pred_prob[:,1]))\n",
    "                \n",
    "if 'lda' in desiredClassifiers:\n",
    "    lda_labels=[];\n",
    "    for i in range(num_feats):\n",
    "        lda_labels.append('lda '+selection_type+\" \"+str(i+1))\n",
    "        try: \n",
    "            lda_sens_ave.append(np.nanmean(lda_sensitivity[i,:]))\n",
    "            lda_sens_std.append(np.nanstd(lda_sensitivity[i,:]))\n",
    "            lda_spec_ave.append(np.nanmean(lda_specificity[i,:]))\n",
    "            lda_spec_std.append(np.nanstd(lda_specificity[i,:]))\n",
    "            lda_auc_ave.append(np.nanmean(lda_auc[i,:]))\n",
    "            lda_auc_std.append(np.nanstd(lda_auc[i,:]))\n",
    "        except:\n",
    "            lda_sens_ave.append(float(\"nan\"))\n",
    "            lda_sens_std.append(float(\"nan\"))\n",
    "            lda_spec_ave.append(float(\"nan\"))\n",
    "            lda_spec_std.append(float(\"nan\"))\n",
    "            lda_auc_ave.append(float(\"nan\"))\n",
    "            lda_auc_std.append(float(\"nan\"))          \n",
    "    lda_scores=pd.DataFrame([lda_sens_ave,lda_sens_std,lda_spec_ave,lda_spec_std,lda_auc_ave,lda_auc_std],columns=lda_labels).T\n",
    "    lda_scores.columns=['sensitivity','sens_std','specificity','spec_std','auc','auc_std']\n",
    "    LDAStats=LDAStats.append(lda_scores)\n",
    "    \n",
    "if 'qda' in desiredClassifiers:\n",
    "    qda_labels=[]    \n",
    "    for i in range(num_feats):\n",
    "        qda_labels.append('qda '+selection_type+\" \"+str(i+1))\n",
    "        try: \n",
    "            qda_sens_ave.append(np.nanmean(qda_sensitivity[i,:]))\n",
    "            qda_sens_std.append(np.nanstd(qda_sensitivity[i,:]))\n",
    "            qda_spec_ave.append(np.nanmean(qda_specificity[i,:]))\n",
    "            qda_spec_std.append(np.nanstd(qda_specificity[i,:]))\n",
    "            qda_auc_ave.append(np.nanmean(qda_auc[i,:]))\n",
    "            qda_auc_std.append(np.nanstd(qda_auc[i,:]))\n",
    "        except:\n",
    "            qda_sens_ave.append(float(\"nan\"))\n",
    "            qda_sens_std.append(float(\"nan\"))\n",
    "            qda_spec_ave.append(float(\"nan\"))\n",
    "            qda_spec_std.append(float(\"nan\"))\n",
    "            qda_auc_ave.append(float(\"nan\"))\n",
    "            qda_auc_std.append(float(\"nan\"))\n",
    "    qda_scores=pd.DataFrame([qda_sens_ave,qda_sens_std,qda_spec_ave,qda_spec_std,qda_auc_ave,qda_auc_std],columns=qda_labels).T\n",
    "    qda_scores.columns=['sensitivity','sens_std','specificity','spec_std','auc','auc_std']\n",
    "    QDAStats=QDAStats.append(qda_scores)\n",
    "    \n",
    "if 'rf' in desiredClassifiers:\n",
    "    rf_labels=[]    \n",
    "    for i in range(num_feats):\n",
    "        rf_labels.append('rf '+selection_type+\" \"+str(i+1))\n",
    "\n",
    "        rf_sens_ave.append(np.nanmean(rf_sensitivity[i,:]))\n",
    "        rf_sens_std.append(np.nanstd(rf_sensitivity[i,:]))\n",
    "        rf_spec_ave.append(np.nanmean(rf_specificity[i,:]))\n",
    "        rf_spec_std.append(np.nanstd(rf_specificity[i,:]))\n",
    "        rf_auc_ave.append(np.nanmean(rf_auc[i,:]))\n",
    "        rf_auc_std.append(np.nanstd(rf_auc[i,:]))\n",
    "\n",
    "    rf_scores=pd.DataFrame([rf_sens_ave,rf_sens_std,rf_spec_ave,rf_spec_std,rf_auc_ave,rf_auc_std],columns=rf_labels).T\n",
    "    rf_scores.columns=['sensitivity','sens_std','specificity','spec_std','auc','auc_std']\n",
    "    RFStats=RFStats.append(rf_scores)\n",
    "    \n",
    "MID_featcounts=MID_featcounts.sort_values(by='counts',ascending=False)\n",
    "MIQ_featcounts=MIQ_featcounts.sort_values(by='counts',ascending=False)\n",
    "RS_featcounts=RS_featcounts.sort_values(by='counts',ascending=False)\n",
    "Ttest_featcounts=Ttest_featcounts.sort_values(by='counts',ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
