{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Net_block.ipynb\n",
    "\n",
    "class Net(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_arch(arch):\n",
    "        net_depth  = len(arch) - 1\n",
    "        layer_list = [None] * net_depth\n",
    "        param_list = [None] * net_depth\n",
    "        \n",
    "        input_dims = arch[0][1]\n",
    "        for i in range(1, net_depth + 1):\n",
    "            layer_list[i-1] = arch[i][0]\n",
    "            param_list[i-1] = arch[i][1]\n",
    "        return input_dims, layer_list, param_list\n",
    "    \n",
    "    @staticmethod\n",
    "    def parse_layers(arch):\n",
    "        input_dims, layer_list, param_list = Net.parse_arch(arch)\n",
    "        net_depth = len(layer_list)\n",
    "        layers = []\n",
    "        for i in range(net_depth):\n",
    "            layers.append(Block_mapping.module_mapping[layer_list[i]](param_list[i]))\n",
    "        return input_dims, layers\n",
    "    \n",
    "    @staticmethod\n",
    "    def init_weights(net, init_type='normal', init_gain=0.02):\n",
    "        \"\"\"Initialize network weights.\n",
    "        Parameters:\n",
    "            net (network)   -- network to be initialized\n",
    "            init_type (str) -- the name of an initialization method: normal | xavier | kaiming | orthogonal\n",
    "            init_gain (float)    -- scaling factor for normal, xavier and orthogonal.\n",
    "        \"\"\"\n",
    "        def init_func(m):\n",
    "            classname = m.__class__.__name__\n",
    "            if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
    "                if init_type == 'normal':\n",
    "                    nn.init.normal_(m.weight.data, 0.0, init_gain)\n",
    "                elif init_type == 'xavier':\n",
    "                    nn.init.xavier_normal_(m.weight.data, gain=init_gain)\n",
    "                elif init_type == 'kaiming':\n",
    "                    nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "                elif init_type == 'orthogonal':\n",
    "                    nn.init.orthogonal_(m.weight.data, gain=init_gain)\n",
    "                else:\n",
    "                    raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
    "                if hasattr(m, 'bias') and m.bias is not None:\n",
    "                    nn.init.constant_(m.bias.data, 0.0)\n",
    "            elif classname.find('BatchNorm2d') != -1:  # BatchNorm Layer's weight is not a matrix; only normal distribution applies.\n",
    "                nn.init.normal_(m.weight.data, 1.0, init_gain)\n",
    "                nn.init.constant_(m.bias.data, 0.0)\n",
    "                \n",
    "        print('initialize network with %s' % init_type)\n",
    "        net.apply(init_func)  # apply the initialization function <init_func>\n",
    "    \n",
    "class Network_template(nn.Module):\n",
    "    \n",
    "    def __init__(self, ngpu, arch):\n",
    "        super(Network_template, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(*nn.ModuleList(arch))\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "        #return torch.squeeze(output)\n",
    "        return output\n",
    "    \n",
    "class StandardLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, mode, reduction):\n",
    "        super(StandardLoss, self).__init__()\n",
    "        self.loss_mode = mode\n",
    "        if mode == 'xentropy':\n",
    "            self.criterion = nn.CrossEntropyLoss(reduction=reduction)\n",
    "        else:\n",
    "            raise NotImplementedError('loss mode %s not implemented' % mode)\n",
    "            \n",
    "    def xentropy_loss(self, params):\n",
    "        # params: [0]net, [1]Dtrain, [2]labels\n",
    "        assert(len(params) == 3)\n",
    "        predictions = params[0](params[1])\n",
    "        loss = self.criterion(predictions, params[2])\n",
    "        return loss\n",
    "    \n",
    "    def __call__(self, params):\n",
    "        if self.loss_mode == 'xentropy':\n",
    "            loss = self.xentropy_loss(params)\n",
    "        return loss\n",
    "    \n",
    "class GANLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, gan_mode, reduction):\n",
    "        super(GANLoss, self).__init__()\n",
    "        self.register_buffer('real_label', torch.tensor(1.0))\n",
    "        self.register_buffer('fake_label', torch.tensor(0.0))\n",
    "        self.gan_mode = gan_mode\n",
    "        if gan_mode == 'lsgan':\n",
    "            self.criterion = nn.MSELoss(reduction=reduction)\n",
    "        elif gan_mode == 'vanilla' or gan_mode == \"vanilla_topo\":\n",
    "            self.criterion = nn.BCEWithLogitsLoss(reduction=reduction)\n",
    "        elif gan_mode == \"wgangp\" or gan_mode == \"wgan\":\n",
    "            pass\n",
    "        else:\n",
    "            raise NotImplementedError('gan mode %s not implemented' % gan_mode)\n",
    "            \n",
    "    def get_target_tensor(self, prediction, is_real):\n",
    "        if is_real:\n",
    "            target_tensor = self.real_label\n",
    "        else:\n",
    "            target_tensor = self.fake_label\n",
    "        return target_tensor.expand_as(prediction)\n",
    "    \n",
    "    def calc_gradient_penalty(self, netD, device, Dreal, Dfake, constant=1.0, lambda_gp=10.0):\n",
    "        batch_size = Dreal.shape[0]\n",
    "        alpha = torch.rand(batch_size, 1, 1, 1, device=device)\n",
    "        alpha = alpha.expand_as(Dreal)\n",
    "        interpolated = alpha * Dreal + (1 - alpha) * Dfake\n",
    "        interpolated.requires_grad_(True)\n",
    "        out_interp = netD(interpolated)\n",
    "        gradients = torch.autograd.grad(outputs=out_interp, inputs=interpolated,\n",
    "                    grad_outputs=torch.ones(out_interp.size()).to(device),\n",
    "                    create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "        gradients  = gradients.view(batch_size, -1)\n",
    "        grad_norm  = torch.sqrt(torch.sum(gradients**2, dim=1) + 1e-12)\n",
    "        gp_penalty = ((grad_norm - constant)**2).mean()\n",
    "        return gp_penalty * lambda_gp\n",
    "    \n",
    "    def vanilla_topo_loss(self, params):\n",
    "        '''\n",
    "        params: Dfake_device, Dfix_device\n",
    "        both Dfake_device and Dfix_device should be output from tanh() layer,\n",
    "        which have values between -1.0 and 1.0.\n",
    "        '''\n",
    "        image_shape = list(params[0].shape)\n",
    "        image_shape = image_shape[-2:]\n",
    "        flat_fake = params[0].view(-1, np.prod(image_shape))\n",
    "        flat_fix  = params[1].view(-1, np.prod(image_shape))\n",
    "        topo_err  = self.criterion(flat_fake, flat_fix)\n",
    "        return topo_err\n",
    "    \n",
    "    def wgan_gp_loss(self, params):\n",
    "        if params[0] == \"G\":\n",
    "            # if G params[1]: netD, params[2]: Dfake\n",
    "            assert(len(params) == 3)\n",
    "            out_fake = params[1](params[2])\n",
    "            err_fake = -out_fake.mean()\n",
    "            return err_fake\n",
    "        elif params[0] == \"D\":\n",
    "            # if D params[1]: netD, params[2]: device, params[3]: Dreal, params[4]: Dfake\n",
    "            assert(len(params) >= 5)\n",
    "            out_real = params[1](params[3])\n",
    "            out_fake = params[1](params[4].detach())\n",
    "            gp_penalty = self.calc_gradient_penalty(params[1], params[2], params[3], params[4].detach())\n",
    "            return out_fake.mean() - out_real.mean() + gp_penalty\n",
    "        else:\n",
    "            raise NotImplementedError('Unrecognized network' % params[0])\n",
    "            \n",
    "    def wgan_loss(self, params):\n",
    "        if params[0] == \"G\":\n",
    "            # if G params[1]: netD, params[2]: Dfake\n",
    "            assert(len(params) == 3)\n",
    "            out_fake = params[1](params[2])\n",
    "            err_fake = -out_fake.mean()\n",
    "            return err_fake\n",
    "        elif params[0] == \"D\":\n",
    "            # if D params[1]: netD, params[2]: device, params[3]: Dreal, params[4]: Dfake\n",
    "            assert(len(params) >= 5)\n",
    "            out_real = params[1](params[3])\n",
    "            out_fake = params[1](params[4].detach())\n",
    "            return out_fake.mean() - out_real.mean()\n",
    "        else:\n",
    "            raise NotImplementedError('Unrecognized network' % params[0])\n",
    "            \n",
    "    def gan_loss(self, params):\n",
    "        if params[0] == \"G\":\n",
    "            # if G params[1]: netD, params[2]: Dfake\n",
    "            assert(len(params) == 3)\n",
    "            out_fake      = params[1](params[2])\n",
    "            target_tensor = self.get_target_tensor(out_fake, True)\n",
    "            err_fake      = self.criterion(out_fake, target_tensor)\n",
    "            return err_fake\n",
    "        elif params[0] == \"D\":\n",
    "            # if D params[1]: netD, params[2]: device, params[3]: Dreal, params[4]: Dfake\n",
    "            assert(len(params) >= 5)\n",
    "            out_real      = params[1](params[3])\n",
    "            out_fake      = params[1](params[4].detach())\n",
    "            target_tensor = self.get_target_tensor(out_real, True)\n",
    "            err_real      = self.criterion(out_real, target_tensor)\n",
    "            target_tensor = self.get_target_tensor(out_fake, False)\n",
    "            err_fake      = self.criterion(out_fake, target_tensor)\n",
    "            return err_real + err_fake\n",
    "        else:\n",
    "            raise NotImplementedError('Unrecognized network' % params[0])\n",
    "        \n",
    "    def __call__(self, params):\n",
    "        if self.gan_mode in ['lsgan', 'vanilla']:\n",
    "            loss = self.gan_loss(params)\n",
    "        elif self.gan_mode == 'vanilla_topo':\n",
    "            loss = self.vanilla_topo_loss(params)\n",
    "        elif self.gan_mode == 'wgangp':\n",
    "            loss = self.wgan_gp_loss(params)\n",
    "        elif self.gan_mode == \"wgan\":\n",
    "            loss = self.wgan_loss(params)\n",
    "        return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
